# ##################################################################################### #
# ì´ ìŠ¤í¬ë¦½íŠ¸ëŠ” IBM Watson Studio Pipelinesì™€ í˜¸í™˜ë˜ë„ë¡ ë¦¬íŒ©í† ë§ë˜ì—ˆìŠµë‹ˆë‹¤.             #
# ê° í•¨ìˆ˜ëŠ” íŒŒì´í”„ë¼ì¸ì˜ í•œ ë‹¨ê³„ë¥¼ ë‚˜íƒ€ë‚´ë©°, ë…ë¦½ì ì¸ ë…¸íŠ¸ë¶ ì…€ì— ë³µì‚¬í•˜ì—¬ ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. #
# ì „ì—­ ìƒíƒœ(global state)ë¥¼ ì œê±°í•˜ê³  í•¨ìˆ˜ ê°„ ë°ì´í„° ì „ë‹¬ì„ ëª…í™•íˆ í•˜ì—¬ ëª¨ë“ˆì„±ì„ ë†’ì˜€ìŠµë‹ˆë‹¤.    #
# ##################################################################################### #

# ############## #
# 01. ì´ˆê¸°í™” #
# ############## #
from ibm_watsonx_ai import Credentials, APIClient
from ibm_watsonx_ai.foundation_models import Embeddings
from ibm_watsonx_ai.metanames import EmbedTextParamsMetaNames as EmbedParams
from dotenv import load_dotenv
from pymilvus import connections, Collection, FieldSchema, CollectionSchema, DataType, utility
import os
import pandas as pd
from langchain_text_splitters import RecursiveCharacterTextSplitter
from pathlib import Path
from ibm_watsonx_ai.foundation_models import ModelInference
from ibm_watsonx_ai.metanames import GenTextParamsMetaNames as GenParams
from ibm_watsonx_ai.foundation_models import Rerank
import json
from datetime import datetime
from ibm_watson_studio_pipelines import WSPipelines

def initialize_pipeline():
    """ëª¨ë“  ì—°ê²° ë° êµ¬ì„±ì„ ì´ˆê¸°í™”í•˜ê³  Watson Pipelines í´ë¼ì´ì–¸íŠ¸ë¥¼ ì„¤ì •í•©ë‹ˆë‹¤."""
    try:
        load_dotenv()
        
        api_key = os.getenv("API_KEY")
        project_id = os.getenv("PROJECT_ID")
        watsonx_url = os.getenv("WATSONX_URL")
        
        config = {
            "api_key": api_key,
            "project_id": project_id,
            "watsonx_url": watsonx_url,
            "milvus_host": os.getenv("MILVUS_HOST"),
            "milvus_port": os.getenv("MILVUS_PORT"),
            "milvus_user": os.getenv("MILVUS_USERNAME"),
            "milvus_password": os.getenv("MILVUS_PASSWORD"),
            "milvus_collection": os.getenv("MILVUS_COLLECTION"),
            "embedding_model": os.getenv("WATSONX_EMBEDDING_MODEL", "ibm/granite-embedding-278m-multilingual"),
            "generation_model": "meta-llama/llama-3-3-70b-instruct"
        }
        
        credentials = Credentials(url=config["watsonx_url"], api_key=config["api_key"])
        client = APIClient(credentials=credentials)
        pipelines_client = WSPipelines.from_apikey(apikey=config["api_key"])
        
        print("âœ… Initialization completed successfully")
        
        return {
            "config": config,
            "credentials": credentials,
            "client": client,
            "pipelines_client": pipelines_client
        }
        
    except Exception as e:
        print(f"âŒ Initialization failed: {e}")
        raise

# ############## #
# 02. íŒŒë¼ë¯¸í„° ì²´í¬ #
# ############## #
def validate_parameters(config):
    """í•„ìš”í•œ ëª¨ë“  ë§¤ê°œë³€ìˆ˜ì™€ êµ¬ì„±ì„ ê²€ì¦í•©ë‹ˆë‹¤."""
    try:
        required_params = [
            "api_key", "project_id", "watsonx_url", 
            "milvus_host", "milvus_port", "milvus_user", 
            "milvus_password", "milvus_collection"
        ]
        
        missing_params = [p for p in required_params if not config.get(p)]
        if missing_params:
            raise Exception(f"Missing required parameters: {missing_params}")
        
        connections.connect(
            host=config["milvus_host"],
            port=config["milvus_port"],
            user=config["milvus_user"],
            password=config["milvus_password"],
            secure=True
        )
        if not connections.has_connection("default"):
            raise Exception("Failed to connect to Milvus")
        connections.disconnect("default")
        
        print("âœ… Parameter validation completed successfully")
        return {"validation_status": "success"}
        
    except Exception as e:
        print(f"âŒ Parameter validation failed: {e}")
        raise

# ############## #
# 03. ì„ë² ë”© #
# ############## #
def process_query_embedding(user_query, config, credentials, project_id):
    """ì‚¬ìš©ì ì¿¼ë¦¬ë¥¼ ì²˜ë¦¬í•˜ê³  ì„ë² ë”©ì„ ìƒì„±í•©ë‹ˆë‹¤."""
    try:
        embedding_params = {
            EmbedParams.TRUNCATE_INPUT_TOKENS: 1024,
            EmbedParams.RETURN_OPTIONS: {"input_text": True}
        }
        
        embeddings = Embeddings(
            model_id=config["embedding_model"],
            params=embedding_params,
            credentials=credentials,
            project_id=project_id,
            batch_size=1000,
            concurrency_limit=5
        )
        
        print(f"Generating embedding for query: {user_query}")
        query_embedding = embeddings.embed_query(text=user_query)
        embedding_dim = len(query_embedding)
        
        print(f"âœ… Embedding completed - Vector length: {embedding_dim}")
        
        return {
            "user_query": user_query,
            "query_embedding": query_embedding,
            "embedding_dim": embedding_dim
        }
        
    except Exception as e:
        print(f"âŒ Query embedding failed: {e}")
        raise

# ############## #
# 04. ë²¡í„°ë””ë¹„ ê²€ìƒ‰ #
# ############## #
def create_sample_collection(config, credentials, project_id, embedding_dim):
    """í…ŒìŠ¤íŠ¸ìš© ìƒ˜í”Œ ì»¬ë ‰ì…˜ì„ ìƒì„±í•©ë‹ˆë‹¤."""
    if utility.has_collection(config["milvus_collection"]):
        utility.drop_collection(config["milvus_collection"])
        print(f"Dropped existing collection: {config['milvus_collection']}")
    
    fields = [
        FieldSchema(name="id", dtype=DataType.INT64, is_primary=True, auto_id=True),
        FieldSchema(name="text", dtype=DataType.VARCHAR, max_length=2048),
        FieldSchema(name="embedding", dtype=DataType.FLOAT_VECTOR, dim=embedding_dim)
    ]
    schema = CollectionSchema(fields=fields, description="Sample collection for RAG pipeline")
    collection = Collection(name=config["milvus_collection"], schema=schema)
    print(f"Created collection: {config['milvus_collection']}")
    
    sample_docs = [
        "Artificial intelligence is a branch of computer science that aims to create intelligent machines.",
        "Machine learning is a subset of artificial intelligence that enables computers to learn from experience.",
        "Deep learning is a type of machine learning that uses neural networks with multiple layers.",
        "Natural language processing focuses on the interaction between computers and human language.",
        "Computer vision enables machines to interpret and understand visual information.",
    ]
    
    embeddings_model = Embeddings(
        model_id=config["embedding_model"],
        credentials=credentials,
        project_id=project_id
    )
    sample_embeddings = embeddings_model.embed_documents(texts=sample_docs)
    
    collection.insert([sample_docs, sample_embeddings])
    collection.flush()
    print(f"Inserted {len(sample_docs)} sample documents.")
    
    index_params = {"metric_type": "COSINE", "index_type": "HNSW", "params": {"M": 16, "efConstruction": 200}}
    collection.create_index(field_name="embedding", index_params=index_params)
    collection.load()
    print(f"Collection '{config['milvus_collection']}' created and loaded with {collection.num_entities} entities.")
    return collection

def search_vector_database(config, credentials, project_id, query_embedding, embedding_dim, limit=5):
    """ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ì—ì„œ ìœ ì‚¬í•œ ë¬¸ì„œë¥¼ ê²€ìƒ‰í•©ë‹ˆë‹¤."""
    try:
        print(f"Connecting to Milvus at {config['milvus_host']}:{config['milvus_port']}")
        connections.connect(
            host=config["milvus_host"],
            port=config["milvus_port"],
            user=config["milvus_user"],
            password=config["milvus_password"],
            secure=True
        )
        
        recreate_collection = not utility.has_collection(config["milvus_collection"])
        if not recreate_collection:
            collection = Collection(config["milvus_collection"])
            if collection.num_entities == 0:
                print("Collection is empty, recreating.")
                recreate_collection = True
        
        if recreate_collection:
            print("Collection not found or empty. Creating a sample collection.")
            collection = create_sample_collection(config, credentials, project_id, embedding_dim)
        else:
            collection = Collection(config["milvus_collection"])
            if not collection.has_index():
                index_params = {"metric_type": "COSINE", "index_type": "HNSW", "params": {"M": 16, "efConstruction": 200}}
                collection.create_index(field_name="embedding", index_params=index_params)
                collection.flush()
            collection.load()

        search_params = {"metric_type": "COSINE", "params": {"ef": 64}}
        results = collection.search(
            data=[query_embedding],
            anns_field="embedding",
            param=search_params,
            limit=limit,
            output_fields=["text"]
        )
        
        search_results = [{
            "similarity": float(1 - hit.distance),
            "content": hit.entity.get("text", ""),
            "id": hit.id,
            "distance": float(hit.distance)
        } for hits in results for hit in hits]
        
        print(f"\nSearch Results ({len(search_results)} found):")
        for i, result in enumerate(search_results[:3]):
            print(f"  {i+1}. Similarity: {result['similarity']:.4f}, Content: {result['content'][:100]}...")
            
        collection.release()
        connections.disconnect("default")
        
        print(f"âœ… Vector search completed - Found {len(search_results)} results")
        return {"search_results": search_results}
        
    except Exception as e:
        print(f"âŒ Vector search failed: {e}")
        if 'collection' in locals():
            try:
                collection.release()
                connections.disconnect("default")
            except: pass
        raise

# ############## #
# 05. ë¦¬ë­í‚¹ #
# ############## #
def rerank_results(user_query, search_results, config, credentials, project_id):
    """ì¬ìˆœìœ„ ì§€ì • ëª¨ë¸ì„ ì‚¬ìš©í•˜ì—¬ ê²€ìƒ‰ ê²°ê³¼ë¥¼ ì¬ì •ë ¬í•©ë‹ˆë‹¤."""
    try:
        if not search_results:
            print("No search results to rerank")
            return {"reranked_results": []}
        
        rerank = Rerank(
            model_id="cross-encoder/ms-marco-minilm-l-12-v2",
            credentials=credentials,
            project_id=project_id
        )
        documents = [result["content"] for result in search_results]
        
        print(f"Reranking {len(documents)} documents...")
        rerank_response = rerank.generate(query=user_query, inputs=documents)
        
        reranked_results = []
        if "results" in rerank_response:
            for i, rerank_item in enumerate(rerank_response["results"]):
                if i < len(search_results):
                    result = search_results[i].copy()
                    result["rerank_score"] = rerank_item.get("score", 0)
                    reranked_results.append(result)
            reranked_results.sort(key=lambda x: x["rerank_score"], reverse=True)
        else:
            reranked_results = search_results
        
        print(f"âœ… Reranking completed - {len(reranked_results)} results reranked")
        return {"reranked_results": reranked_results}
        
    except Exception as e:
        print(f"âš ï¸ Reranking failed, using original search results: {str(e)}")
        return {"reranked_results": search_results}

# ############## #
# 06. ìƒì„± #
# ############## #
def generate_response(user_query, reranked_results, config, credentials, project_id):
    """ê²€ìƒ‰ëœ ì»¨í…ìŠ¤íŠ¸ë¥¼ ì‚¬ìš©í•˜ì—¬ ìµœì¢… ì‘ë‹µì„ ìƒì„±í•©ë‹ˆë‹¤."""
    try:
        context_docs = reranked_results[:3]
        context = "\n\n".join([f"Document {i+1}: {doc['content']}" for i, doc in enumerate(context_docs)])
        
        generate_params = {GenParams.MAX_NEW_TOKENS: 500, GenParams.TEMPERATURE: 0.7, GenParams.TOP_P: 0.9}
        
        chat_model = ModelInference(
            model_id=config["generation_model"],
            params=generate_params,
            credentials=credentials,
            project_id=project_id
        )
        
        system_prompt = "You are a helpful AI assistant. Use the provided context documents to answer the user's question. If the context doesn't contain enough information, say so clearly. Always cite which document(s) you used for your answer."
        user_prompt = f"Context Documents:\n{context}\n\nQuestion: {user_query}\n\nPlease provide a comprehensive answer based on the context provided above."
        messages = [{"role": "system", "content": system_prompt}, {"role": "user", "content": user_prompt}]
        
        print("Generating response...")
        response = chat_model.chat(messages=messages)
        generated_text = response['choices'][0].get('message', {}).get('content', '') if response and "choices" in response and response["choices"] else ""
        
        final_response = {
            "query": user_query,
            "context_docs": context_docs,
            "generated_response": generated_text,
            "metadata": {"model": config["generation_model"], "context_count": len(context_docs)}
        }
        
        print(f"âœ… Response generation completed - {len(generated_text)} characters generated")
        return {"final_response": final_response}
        
    except Exception as e:
        print(f"âŒ Response generation failed: {e}")
        raise

# ############## #
# 07. ê²°ê³¼ ì €ì¥ #
# ############## #
def save_final_results(output_file, final_response, config, intermediate_results):
    """ìµœì¢… ê²°ê³¼ì™€ íŒŒì´í”„ë¼ì¸ ìƒíƒœë¥¼ ì €ì¥í•©ë‹ˆë‹¤."""
    try:
        output_data = {
            "pipeline_execution_summary": {"timestamp": datetime.now().isoformat()},
            "final_response": final_response,
            "pipeline_inputs": {
                "user_query": final_response.get("query"),
                "config": {k: v for k, v in config.items() if k not in ["api_key", "milvus_password"]},
            },
            "intermediate_results_summary": {
                "query_embedding_length": intermediate_results.get("embedding_dim"),
                "search_results_count": len(intermediate_results.get("search_results", [])),
                "reranked_results_count": len(intermediate_results.get("reranked_results", []))
            }
        }
        
        output_path = Path(output_file)
        with open(output_path, 'w', encoding='utf-8') as f:
            json.dump(output_data, f, indent=2, ensure_ascii=False)
        
        print(f"âœ… Final results saved to {output_path}")
        return output_path
        
    except Exception as e:
        print(f"âŒ Saving results failed: {e}")
        raise

# ############## #
# ë©”ì¸ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ #
# ############## #
def run_full_pipeline(user_query, output_file="pipeline_results.json"):
    """ì „ì²´ RAG íŒŒì´í”„ë¼ì¸ì„ ì‹¤í–‰í•˜ê³  Watson Studio Pipelines í†µí•©ì„ ì‹œë®¬ë ˆì´ì…˜í•©ë‹ˆë‹¤."""
    print("ğŸš€ Starting RAG Pipeline Execution (Watson Pipelines Simulation)")
    print("=" * 60)
    
    try:
        # ğŸ“‹ Stage 1: Initialization
        # ì´ ë…¸íŠ¸ë¶ì€ ì´ˆê¸° êµ¬ì„±ì„ ë¡œë“œí•˜ê³  í´ë¼ì´ì–¸íŠ¸ë¥¼ ì„¤ì •í•©ë‹ˆë‹¤.
        print("\nğŸ“‹ Stage 1: Initialization")
        init_results = initialize_pipeline()
        config = init_results["config"]
        credentials = init_results["credentials"]
        pipelines_client = init_results["pipelines_client"]
        project_id = config["project_id"]
        # >> Watson Pipeline: ì´ ë‹¨ê³„ì˜ ê²°ê³¼(ì˜ˆ: ìƒíƒœ)ë¥¼ ì €ì¥í•©ë‹ˆë‹¤.
        # pipelines_client.store_results({"initialization_status": "success"})
        
        # ğŸ” Stage 2: Parameter Validation
        # ì´ ë…¸íŠ¸ë¶ì€ êµ¬ì„± ê°’ì˜ ìœ íš¨ì„±ì„ ê²€ì‚¬í•©ë‹ˆë‹¤.
        print("\nğŸ” Stage 2: Parameter Validation")
        validate_parameters(config)
        # >> Watson Pipeline: ê²€ì¦ ìƒíƒœë¥¼ ì €ì¥í•©ë‹ˆë‹¤.
        # pipelines_client.store_results({"validation_status": "success"})
        
        # ğŸ”¤ Stage 3: Query Embedding
        # ì´ ë…¸íŠ¸ë¶ì€ ì‚¬ìš©ì ì¿¼ë¦¬ë¥¼ ì…ë ¥ë°›ì•„ ì„ë² ë”© ë²¡í„°ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.
        print("\nğŸ”¤ Stage 3: Query Embedding")
        embedding_output = process_query_embedding(user_query, config, credentials, project_id)
        # >> Watson Pipeline: ìƒì„±ëœ ì„ë² ë”©ê³¼ ì°¨ì›ì„ ë‹¤ìŒ ë‹¨ê³„ë¥¼ ìœ„í•´ ì €ì¥í•©ë‹ˆë‹¤.
        # pipelines_client.store_results(embedding_output)
        
        # ğŸ” Stage 4: Vector Database Search
        # ì´ ë…¸íŠ¸ë¶ì€ ì„ë² ë”©ì„ ì‚¬ìš©í•˜ì—¬ ë²¡í„° DBì—ì„œ ê´€ë ¨ ë¬¸ì„œë¥¼ ê²€ìƒ‰í•©ë‹ˆë‹¤.
        print("\nğŸ” Stage 4: Vector Database Search")
        search_output = search_vector_database(config, credentials, project_id, embedding_output["query_embedding"], embedding_output["embedding_dim"])
        # >> Watson Pipeline: ê²€ìƒ‰ ê²°ê³¼ë¥¼ ì €ì¥í•©ë‹ˆë‹¤.
        # pipelines_client.store_results(search_output)
        
        # ğŸ“Š Stage 5: Result Reranking
        # ì´ ë…¸íŠ¸ë¶ì€ ê²€ìƒ‰ ê²°ê³¼ë¥¼ ì¬ìˆœìœ„í™”í•˜ì—¬ ê´€ë ¨ì„±ì„ ë†’ì…ë‹ˆë‹¤.
        print("\nğŸ“Š Stage 5: Result Reranking")
        rerank_output = rerank_results(user_query, search_output["search_results"], config, credentials, project_id)
        # >> Watson Pipeline: ì¬ìˆœìœ„í™”ëœ ê²°ê³¼ë¥¼ ì €ì¥í•©ë‹ˆë‹¤.
        # pipelines_client.store_results(rerank_output)
        
        # ğŸ’­ Stage 6: Response Generation
        # ì´ ë…¸íŠ¸ë¶ì€ ì¬ìˆœìœ„í™”ëœ ê²°ê³¼ë¥¼ ì»¨í…ìŠ¤íŠ¸ë¡œ ì‚¬ìš©í•˜ì—¬ ìµœì¢… ë‹µë³€ì„ ìƒì„±í•©ë‹ˆë‹¤.
        print("\nğŸ’­ Stage 6: Response Generation")
        generation_output = generate_response(user_query, rerank_output["reranked_results"], config, credentials, project_id)
        # >> Watson Pipeline: ìµœì¢… ìƒì„±ëœ ì‘ë‹µì„ ì €ì¥í•©ë‹ˆë‹¤.
        # pipelines_client.store_results(generation_output)

        # ğŸ’¾ Stage 7: Save Final Results (Local)
        # ì´ ë‹¨ê³„ëŠ” íŒŒì´í”„ë¼ì¸ì˜ ìµœì¢… ê²°ê³¼ë¥¼ ë¡œì»¬ íŒŒì¼ì— ìš”ì•½í•˜ì—¬ ì €ì¥í•©ë‹ˆë‹¤.
        print("\nğŸ’¾ Stage 7: Save Final Local Results")
        final_response = generation_output["final_response"]
        save_final_results(
            output_file, 
            final_response, 
            config, 
            {
                "embedding_dim": embedding_output["embedding_dim"],
                "search_results": search_output["search_results"],
                "reranked_results": rerank_output["reranked_results"]
            }
        )
        
        print("\n" + "=" * 60)
        print("ğŸ‰ Pipeline execution completed successfully!")
        
        print(f"\nğŸ“ Query: {final_response['query']}")
        print(f"\nğŸ¤– Response:\n{final_response['generated_response']}")
        
        return final_response
        
    except Exception as e:
        print(f"\nâŒ Pipeline execution failed: {e}")
        import traceback
        traceback.print_exc()
        raise

if __name__ == "__main__":
    test_query = "What is artificial intelligence and how does it relate to machine learning?"
    
    try:
        result = run_full_pipeline(test_query)
    except Exception as e:
        print(f"\nTop-level error caught: Pipeline failed.")